---
layout: post
title: ESL笔记「第二章」
ubtitle: 何时才能看完这本统计圣经...
date: 2016-02-02
author:     "Norris"
categories: blog
tags: [Statistic]
---

# Overview of Supervised Learning

## 2.2 变量类型
变量的类型：定序 定类 定距 定比

---

## 2.3 最小二乘与$$K$$邻近
最小二乘法的假设更加严格，而结果更加稳定，但是偏差可能比较大。

K邻近的假设非常少，结果比较准确（随着k的变小，结果越来越精确），但是其稳定性较差（随着k的升高，越来越稳定）

### 2.3.1 最小二乘法
最小二乘法做分类时更适合数据来源于Bivariate Gaussian分布，并且相互不相关，拥有不同的均值（此情形之后称为情景一）

### 2.3.2 K邻近

K邻近算法用目标x周围最近的k个x的函数值的平均来表示目标x的值。

$$\hat{Y}(x)=\frac{1}{k}\sum_{x_i \in N_k(x)}y_i$$

通过邻近的这个概念是一种距离的测度，常用的距离为欧几里得距离。

K邻近的方法更适用于数据来源比较复杂的情况，例如来源于10个混合的低方差高斯分布，其中每个高斯分布的均值来源于同一个高斯分布(此情形之后称为情景二)。

K邻近方法需要计算的有效参数值个数大多数情况下比最小二乘法多，而且不能使用损失函数来惩罚。

k的大小有一个trade-off在里面，虽然k越小越精确，但是稳定性会越来越差，所以要选择合适的k。

### 2.3.3 最小二乘与K邻近的比较

**一个小的总结：**

* 最小二乘的方法得到的decision boundary是非常平滑的，它依赖于比较强的假设条件，得到的结果一般来讲variance较小，而bias比较高。更适用于情景一。

* K邻近方法不需要数据的强假设，可是适用于任何情形，虽然其得到的结果非常精确，既Bias较低，但是其variance一般是比较大的。更适用于情景二。

----

## 2.4 统计决策理论

什么是最优的预测值？

### 2.4.1 输出是连续变量的情况

如果使用MSE作为Loss Function的话，那么最好的预测就是基于观测值的条件期望。

Nearest-neighbor的方法更直接，它有两个近似：

1. 用平均值替代均值
2. 用目标x周围的邻近点来替代条件期望。

随着样本量N的增加，目标x的邻近点离x越来越近。随着k的增加，均值也越来越稳定。也就是说随着N和k的增加，K邻近的预测值将趋近于利用MSE作为损失函数的条件期望。

<b>虽然如此，但是随着dimension p的增加，收敛依旧成立，其收敛速度会随着p的增加而减慢。</b>

线性回归的方法直接将regression function近似为线性的。

虽然k邻近和最小二乘方法都是利用平均化的思想来估计条件期望，但是他们的假设却别非常大：

1. 最小二乘法假设f(x)总体是一个线性的方程
2. k邻近假设f(x)又一个局部的常数方程决定

### 2.4.2 输出为分类变量的情况

在输出变为离散的情况下，我们定义一个Loss Function来惩罚分类出现的错误。再对所有类的Loss Function的求期望，使得这个期望最小的分类就是所估计的分类结果。

通过求解发现，结果很容易理解为，在给定了输入的情况下，使得出现类的概率最大的一类即所估计的类。这个分类方法就是Bayes classifier，而bayes classifier分类的错误率就成为bayes rate。

对于k邻近的方法，与连续的情况类似，利用平均值来近似期望， 利用x周围k个邻近点来近似条件希望。

---

## 2.5 高维下的局部方法

虽然KNN方法在样本量增大的情况下有着较好的逼近性质，但是在高维情况下也会有许多问题出现。

- 高维情况下，局部的方法将不怎么局部。 $$r^{1/p}$$

- 高维情况下，样本点更趋向于集中于样本的边界。

- 高维情况下，实际的样本将变得非常稀疏。

---

## 2.6 统计模型、监督学习、函数估计

2.4节我们已经知道了，通过最小化Loss Function的期望来估计模型函数f(x)，常见的有最小二乘法和KNN，但是这种方法也有一定的缺陷：

1. 如果是在高维情况下，KNN方法与估计点的距离不如预想中的近（2.5节 第二种情形）

2. 如果输入的数据有特殊的结构存在，那么就可以同时降低偏差和方差。

### 2.6.1 统计模型

Additive error model

$$Y=f(X)+\epsilon$$

随机误差项$$\epsilon$$是零均值的，而且与$$X$$独立。

这个模型适用于输出为连续型变量，而对于离散型（定性变量）更加直接，$$f(X)$$直接用$$Pr(G\|X)$$代替即可。

### 2.6.2 监督学习

Supervised learning attempts to learn $$f$$ by example through a teacher.(ESL page 29)

### 2.6.3 函数估计

最大似然估计

---

## 2.7 结构化回归

可以看出局部方法有诸多缺陷，为了解决这些问题，出现了结构化回归方法。

### 2.7.1 方法的难点

考虑到RSS条件：

$$ RSS(f)=\sum_{i=1}^{N}(y_i-f(x_i))^2$$

只要经过点$$(x_i,y_i)$$的$$f(x)$$都是RSS的一个解，可见这种解有无数多个，为了确定一个更小范围的解，就需要多加一些限制条件。

没有哪一种解法是最优的解法，要根据不同的情况来自己决定。

通常来说，所有解法都是基于同一个思想，对于距离x足够小的范围内的点实行同一种规则，例如近似常数、线性或者低阶多项式等等。

限制的力度取决于neighborhood size，规模越大，约束的能力越强，同时也表明该方法对不同的约束会更敏感。

**[?]** 限制的性质一方面也取决于所使用的metric。可以简单分为explicitly和implicitly两种。

**有一点必须知道的是，如果在任何各项同性的领域内使用可变的方程，那么同样会产生高维的问题。并且，所有克服了高维问题的方法基本上都不允许领域在各个维度上同时变小。**

---

## 2.8 约束的方法分类

2.7节所说的限制的种类大致可以分为几个类，而大多数方法都可以归结在这几类或这几类的组合中。

### 2.8.1 添加惩罚项的方法

这种方法有时候也称为正则化方法。

### 2.8.2 核函数方法

核方程对于目标x周围的x给予不同的权重。

### 2.8.3 基函数方法

这个是对线性模型的推广，假设模型仍然是可加的，对系数而言仍然是线性的，但是所加的这些项则变成了函数（基函数），对这种方法而言，比较常用的一个是样条，另外一个是多项式回归。

---

## 2.9 模型选择

模型选择要基于test sample，主要是在bias与variance的tradeoff中选择一个度，使得expected prediction error达到最小。

