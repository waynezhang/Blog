---
layout: post
title: 再看数据额挖掘--EM算法
subtitle: 本系列属原创 转载请注明原著
date: 2016-03-11
author:     "Norris"
categories: blog
tags: [data mining]
---

数据挖掘十大算法之一的EM（exception maximization）算法的大名可是如雷贯耳，EM算法简单来说，就是由两部分组成：E和M。E是exception，即求期望；M是maximization，求极大值。

为什么要使用EM算法呢？

我们常见的概率模型一般都只有观测变量，即都是给定的数据，可以直接用[极大似然估计、贝叶斯估计](http://blog.csdn.net/yangliuy/article/details/8296481)等方法。但是有些时候，概率模型是含有隐变量或潜在变量的，这时候就是EM算法发挥的时刻了。

先来说说隐变量，隐变量就是无法直接从给定的数据中观察到的变量，比如「统计学习方法」一书中所说的例子：三个筛子A,B,C进行如下实验，先投掷A硬币，根据其结果选出B或C硬币，比如正面选B，反面选C，然后掷出选择到的硬币，如果出出现正面记作1，出现反面记作0，独立重复n次，得到观测结果如下：

> 1,1,0,1,0,0,1,0,1,1

那么如何估计三个硬币正面出现的概率呢？

从结果来看，只能看到B和C的结果，而A得概率是无法直接得到的，此时A正面的概率就是一个这个模型中一个隐变量。

一般的，用$$Y=(Y_1,Y_2,...,Y_n)^T$$表示观测到的随机变量的数据，$$Z=(Z_1,Z_2,...,Z_n)^T$$表示隐随机变量的数据。$$Y和Z$$在一起被称为完全数据，观测数据$$Y$$又称为不完全数据。参数空间为$$\theta$$，假设观测数据Y的概率分布是$$P(Y\|\theta)$$,那么其概率分布是$$P(Y\|\theta)$$，假设$$Y和Z$$的联合概率分布是$$P(Y,Z\|\theta)$$，那么完全数据的对数似然函数就是$$logP(Y,Z\|\theta)$$。

EM算法的步骤就是：

输入：观测数据$$Y$$，隐变量$$Z$$，联合分布$$P(X,Y\|Z)$$，条件分布$$P(Z\|Y,\theta)$$

输出：模型参数$$\theta$$

1. 选择参数的初值$$\theta^{(0)}$$,开始迭代
2. E步：记$$\theta^{(i)}$$为第$$i$$次迭代参数$$\theta$$的估计值，在第$$i+1$$次迭代的E步，计算

	$$\begin{equation}
	\begin{aligned}
	Q(\theta,\theta^{(i)}) &=E_z[logP(Y,Z\|\theta)\|Y,\theta^{(i)}]\\
	 &=\Sigma_zlogP(Y,Z\|\theta)P(Z\|Y,\theta^{(i)})
	\end{aligned}
	\end{equation}$$

3. M步：求使得$$Q(\theta,\theta^{(i)})$$极大化的$$\theta$$，确定第$$i+1$$次迭代的参数值$$\theta^{(i+1)}$$

	$$\theta^{(i+1)}=argmax_{\theta}Q(\theta,\theta^{(i)})$$

4. 重复第2步和第3步，直到收敛。

其中第二步的Q函数（	Q function）是EM算法的核心。

需要注意的是，大多数情况下EM算法是收敛的，但是不能保证收敛到全局最优，可以多选几个初值，通过比较最后得到的参数，选定一个最符合实际的结果。高斯混合模型以及隐马尔科夫模型都是EM算法的重要应用，主要应用于含有隐变量的概率模型的学习。

EM算法其实还可以解释为F函数的极大-极大算法。EM算法的一次迭代可以由F函数的极大-极大算法实现。而EM算法也有很多自己的推广，例如GEM（generalized exception maximization）。GEM也包含多种算法，例如使用F函数的算法，有的时候Q函数的极大化是不那么容易求出来的，所以我们放松EM算法的第3步，将极大化放松为，只要比原来的似然函数大即可，或者换一种思路，我们不直接求$$\theta$$使得Q函数最大，而是把$$\theta$$中每一个分量，依次求使得Q函数最大的结果，然后组合起来（具体来说就是，首先除了第一个的分量，保持其他值不变，求解使得Q函数极大的第一个分量，然后替换到原始的第一个分量，然后除了第二个分量，其余分量保持不变，注意此时第一个分量已经改变了，然后求解使得Q函数极大的第二个分量，替换原始的第二个分量，依次类推，知道所有分量都已经被替换了一遍）。