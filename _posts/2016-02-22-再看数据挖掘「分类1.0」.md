---
layout: post
title: 再看数据挖掘--分类1.0
subtitle: 本系列属原创 转载请注明原著
date: 2016-02-22
author:     "Norris"
categories: blog
tags: [data mining]
---

# 1.决策树

OK，菜都洗好了，就准备一下做什么菜吧。菜谱可是很重要的。从这开始就来讲讲各种常见的算法吧。先从决策树开始讲起。

决策树是在做什么呢，就是分类，分类又可以干什么呢？看了第一节的话，大家应该知道就是预测和描述两个功能。

解决分类问题的一般方法就是给定一个训练集，来训练一个分类模型，然后再用检验集来评估其性能，可以用混淆矩阵的概念来做。

决策树的概念非常形象，就像一颗倒置的树，从根节点出发，到达不同的内部节点，最后终结在叶节点。

如何构建一个决策树呢？大致的思想可以一句话说明，选取一个属性测试条件，将记录划分成越来越小，越来越纯的子集，使得最后的子集中的所有记录属于同一个类。当然这是最理想的情况，有可能无法选出一个属性测试条件，也有可能不能继续划分子集。

解决决策构建的两个重要问题就是如何分裂训练记录，以及如何停止分裂过程。

首先看如何分裂训练记录，这就要先知道都什么什么样的“分裂”。有的属性只有两个可能的结果，有的属性可能有多个，有的属性是序数属性，其结果不能随意分割，连续属性的分裂方法那就太多了。

那么如何选择最佳的分裂方式呢？

要知道分裂就是为了让子集更纯，如何定义纯呢？我们用熵来定义混乱即可，此外还可以供Gini指数、错误率、信息增益来作为混乱度的评价。但是这样定义的不纯度的度量更有利于具有大量不同值的属性，所以可以采用增益率来作为划分标准。增益率如下定义：

$$ Gain ratio = \frac{\varDelta_{info}}{Split\  Info} $$

其中$$\varDelta_{info}$$为信息增益，$$Split\  Info=-\Sigma^k_{i=1}P(v_i)log_2P(V_i)$$，k是划分的总数。

决策树划分好了，但是很有可能出现了过度拟合的情况。什么情况会导致过分拟合呢？首先是噪声，第二是样本量过小，第三是维度过多（多重选择问题，50个人预测至少预测对10只股票中的8只得概率是0.9399）。

如何处理过度拟合的问题呢？两种方法，先剪枝，当不纯度量增量小于某个阈值的时候就停止分裂。缺点是，阈值很难选，而且即使当前的增益率很低，接下来的字数的增益率有可能很高，剪掉了较好的子树。还有一种方法是后剪枝，后剪枝也有两种常用的方法，用新的叶节点替换子树（子树替换），或用子树中最常见的分支代替子树（子树提升）。

那么一个决策树出现了，如何评价他的性能呢？

1. Holdout 简单来说就是我们最常用的划分训练集和检验集的方法。局限性很多：
	
	a. 用于训练样本的数据变小。

	b. 模型可能高度依赖于训练集与检验集的构成。

	c. 训练集与检验集之间不独立。

2. 随机二次抽样 多次重复的Holdout方法，将准确率取均值即可。

3. Cross-Validation交叉验证。非常常用的方法，将训练集等分成几分，选取其中的一份作为检验集剩下的作为训练集，重复以上步骤知道所有集合都被选作过检验集，然后吧结果平均即可。

4. Boostiong。神一样的Boosting。训练记录采用又放回抽样，没有抽中的记录就成为检验集的一部分。每次抽样计算一次准确率。如何计算模型的准确率呢？这里就不是平均了，是.632 bootstrap（0.632是样本量足够大时，大小为N的自助样本包含原始数据中约63.2%的数据）。它通过组合每个自助样本的准确率和由包含所有标记样本的训练集计算的准确率来构成。

$$ acc_{boot}=\frac{1}{b}\Sigma^b_{i=1}(0.632 \times \epsilon_i + 0.368 \times acc_s)$$

如何比较两个分类器呢？

记住假设检验的威力。两个模型是否有显著差别得看假设检验。决策树准确率是一个二项分布，样本量足够大的时候可以近似为正态，所以假设检验的置信区间可以正态分布来做。比较不同分类方法也是如此。

说了这么多，我们来看看决策树有什么特点吧。

1. 决策树不需要对数据进行任何假设，因为它是一种非参数的方法。

2. 决策树非常简单，易于构建，解释性也比较强。

3. 决策树在面对缺失值以及高维问题时有很好的鲁棒性。

4. 决策树的决策边界全部是平行于坐标轴的，对于某些数据（例如用x+y=1完全分割的数据）的分类能力很低（但是可以利用斜决策树解决，即其测试条件变为一条斜线例如：x+y=1，但是如果是这样还不如直接用线性回归）

5. 不纯度的度量的选择对决策树的影响很小，剪枝对决策树的影响可能会更大。所以如果效果不好、不满意一定要剪枝呀。

6. 找到最佳决策树是NP完全问题。

# 2.分类的其他技术

上一篇讲到了决策树的构建、修剪、性能评价等方面的知识，然而分类方法不仅仅只有决策树，还有许多种方法，这一节主要讲讲其他几个常见的方法。

## 2.1基于规则的分类器

什么是基于规则的分类器呢？简单来说就是一个if...then...的过程，符合一定的规则就将其分到符合这个规则的类中。规则的集合就称作规则集，规则集中的每一条小规则的前提成为规则前件，规则前件有许多个合取项构成，规则的预测类成为规则后件。

分类规则的优劣由两个条件左右，一个是覆盖率，一个是准确率。覆盖率是数据集中出发该规则记录所占的比例，准确率是出发该规则记录中类标号正确的比例。

如何构建一个规则分类器呢？规则集的产生需要满足两个条件：互斥规则和穷举规则。互斥即同一个记录不能同时被两个或以上的规则出发，穷举即属性集的任一组合都被规则集中的规则覆盖。如果这两个规则同时满足，就保证了每一条记录只能被有且只有一个规则触发。

然而现实中这种情况有可能是无法实现的，如果穷尽原则没有满足，我们需要建立一个前件为空的规则作为默认规则来覆盖，这个默认规则指向一个默认类。如果规则不是互斥的，规则的预测可能会相互冲突，有一下两种方法来解决：有序规则和无序规则。有序规则，将规则进行排序，若同时满足一个，则选取排序最高的规则触发。无序规则将同一记录的触发的规则指向的类进行统计，然后采用投票最多的类。无序规则的优势在于其对于噪声或者错误的鲁棒性很强，而有序的非常敏感。而且其建立的模型开销很小，不需要去维护规则的顺序。但是它的计算量很大，每一条记录都要同每一条规则进行匹配。

有序规则的构建有两种常见的方案：基于规则的排序方案和基于类的排序方案。基于规则的排序方案假设该方案之前的规则全部是不成立的，当规则数量很大的时候，在尾部的规则将很难解释。基于类的排序方案是把同一类的规则在规则集中同时出现，同一个类之间规则的相对顺序不重要，主要其中一个被触发就赋予该规则的分类。

说了这么多，那么规则到底是如何提取出来的呢？这里只说明基于类的规则排序方法的提取方法。

首先是直接方法，这里只介绍一下广泛使用的RIPPER算法。对于多类问题，先按类的频率进行排序，假设$$(y_1,y_2,...,y_c)$$是排序后的类，其中$$y_1$$是最不频繁的类，在第一次迭代中，将属于$$y_1$$的样例标记为正例，而把其他类的样例标记为反例，使用顺序覆盖算法产生区分正例和反例的规则（顺序覆盖算法即先找出能够覆盖最多样例的规则，然后将这个规则覆盖的样例删掉，选取剩下集合中覆盖样例最多的规则，如此循环）。接下来提取区分$$y_2$$和其他类的规则。重复该过程，直到剩下类$$y_c$$，此时将$$y_c$$作为默认类。

直接方法过后一定是一个间接方法。间接方法由决策树等分类器构建，然后将其中一些规则简化即可。

那么大家肯定要问了，既然如此不如直接用决策树好了。基于规则的分类器有什么好处呢？就是更容易解释模型。基于规则的分类器实际上相比决策树更加复杂，因为他可以同时出发多个条件，可以构造更加复杂的决策边界。基于规则的分类器，尤其是采用基于类的规则定序方法非常适合处理不平衡的数据集。

## 2.2最近邻分类器

大名鼎鼎的KNN，在ESL的第二章就已经开始介绍了，是一种简便而又实用的算法。将KNN之前不妨看看这句谚语。

> If it looks like a duck, quacks like a duck and walks like a duck, it‘s a duck.

KNN就是这么一种思想，如果某个目标值周围的点都是属于某一个类，那么这个点也是属于这个类的。

KNN算法的关键在于邻近点个数$$k$$的选取，太小了容易受噪声影响，过度拟合，过大的话可能会导致错误分类，而且最邻近也变得不那么“邻近”了。

不多说KNN了，大家有兴趣可以去看[ESL第二章](http://norris-niu.github.io/blog/2016/02/02/ESL-第二章/)的讲解。

## 2.3贝叶斯分类器

说到Bayes，这个都不需要用大名鼎鼎来介绍了，统计有一个分支就是贝叶斯统计。贝叶斯公式想必大家都知道了，它为什么这么牛呢，**因为它把先验知识和从数据中提取出的信息相结合起来了！**大家的先验知识就是我们所说的先验概率，而结合了我们数据中的信息所得到的概率（一般是个条件概率）就是后验概率啦。

那么我们接下来就介绍两种分类器。

### 2.3.1朴素贝叶斯分类器

英文叫做Naive Bayes，钟老师说过这个Naive就是说这个分类器太天真了，不过确实也很好用（没有对的模型，只有有用的模型）。为什么说它天真呢，因为他的假设条件一看就不可能成立，它假设每个属性（每个变量）之间条件独立。什么是条件独立？

$$ P(X|Y,Z)=P(X|Z)$$

以上公式成立即说明X,Z条件独立于Y，上面的公式也可以写成：

$$P(X,Y|Z)=P(X|Z) \times P(Y|Z)$$

朴素贝叶斯的假设条件就可以表示为如下：

$$P(X|Y=y)=\prod\limits_{i=1}^dP(X_i|Y=y)$$

其中$$X=\left\{ X_1,X_2,...,X_d\right\}$$包含d个属性。

那么每个后验概率就是：

$$P(Y|X)=\frac{P(Y)\prod_{i=1}^dP(X_i|Y)}{P(X)}$$

[ESL第二章](http://norris-niu.github.io/blog/2016/02/02/ESL-第二章/)讲到贝叶斯方法通过最小化Loss Function之后，得到结果就是选取某个最大化后验概率条件的类。

所以我们的目标就是最大化上面这个后验概率。如何估计这个后验概率呢？如果目标是离散的情况，可以直接用频率来代替。那么如果是连续变量的话，第一种方法就是预处理中提到的将连续变量离散化，第二种方法就是假定变量服从某个分布，例如正态，然后用样本均值和方差替代正态分布中的参数，再计算出每个样本点对应的概率。如果大家基础比较好，可能会说连续变量的概率密度中某一点的概率为0啊，其实这里是用了一个近似，假设在目标点周围一个足够小的区间内计算概率，近似的用目标点的值带入即可（page 143）。

如果大家实际计算过贝叶斯的问题会发现，这个连乘的概率非常脆弱，若果某一项为0，不管其他项多大，结果都是0，而某个样本出现频率为0不一定代表他真的没有出现，只是没有被观测到。所以如何解决这个问题呢？一种方法是m估计，强行的给他们一个小概率。还有一个方法是Good-Turing Estimate（古德-图灵估计，可以参考「数学之美」page 35），看到过，不知道是否可行，有待验证。

评价Bayes分类器的一个标准就是Bayes error rate，即贝叶斯误差率，是被错误分到别的类下面的样本点的比率（离散情况很好理解，那么连续的呢？）。

朴素Bayes分类器有如下特点：

1. 面对孤立的噪声点和无关属性，Bayes Classifier is robust，噪声点的影响会被平均，而无关变量基本是一个均匀分布，不会影响后验概率的比较。

2. 相关属性会降低Bayes分类器的性能，因为不符合假设呀！

### 2.3.2贝叶斯信念网络（Bayesian belief networks,BBN）

为什么不用Naive了呢？就是因为它太Naive了，天真的让人不敢相信。所以就出现了贝叶斯信念网络，他不要求所有的属性条件都独立，而是允许指定哪些属性条件独立。

贝叶斯信念网络有两个主要的组成部分：一个有向无环图（表示各变量之间的依赖性），一个概率表（将各个结点与其父结点关联起来）。

贝叶斯网络一个重要性质是：贝叶斯网络中的一个结点，如果它的父结点已知，则它条件独立于它的所有非后代结点。

概率表的算法是，如果该点没有父母结点，则表中只包含其先验概率。若含有父结点，则该表包含条件概率
$$P(X\|Y_1,Y_2,...,Y_k)$$

建立一个BNN首先需要用“专家”知识对所有的属性进行排序，然后从头开始，每个属性将其前面的所有属性作为条件，然后剔除无关的条件，根据最后的条件概率画出有向图。

可以看出BNN的先决条件是属性的排序，不同的排序将会影响结果，所以这也是BNN的缺陷之一，如何解决呢？就是尽量将属性分为原因与结果，这样就不会有太多的排序可能出现，这就需要“专家”的力量啦！

BNN有什么特点呢？

首先可以看出网络的构建是十分复杂的，但是一旦构建起来添加新变量也非常容易。适合处理数据不完整的情况，各种概率的转换达到估计该遗漏值。由于结合了先验知识，对于过度拟合的鲁棒性是很强的。

## 2.4 人工神经网络

神经网络，听着就有黑科技的感觉！其实他的原理也是很简单的，为什么叫做神经网络呢？其实它是模仿了神经元之间信息的传递。

说神经网络之前，先来说说最简单的一个模型：感知器（perceptron)。

感知器包含两个结点，一个是输入结点，一个是输出结点。每个输入结点通过一定的权值链接到输出结点。这个加权的链的作用是来模拟神经元间神经键链接的强度。训练一个感知器，就相当与不断的调整链接的权值，直到能拟合训练数据的输入输出为止。输出不单单是简单的加权，而是通过一个激活函数（activation function），选择不同的阈值分到不同的类。感知器常用的激活函数就是最简单的符号函数。感知器具体是如何学习的呢？通过一个学习率的加权乘以它的误差即可。注意对于感知器这个算法不一定会收敛(因为只有一层，类似只能用一个超平面来划分数据，像XOR分类的问题是不能通过一个超平面来划分的)。

知道了感知器那么就可以来看看ANN了。有什么区别呢？就是变得更复杂了，复杂在哪里呢？

首先是输入层和输出层之前出现了很多隐藏层，根据隐藏层中神经元的连接方式，可以分为前馈神经网络（每一层的结点仅和下一层的结点相连）和递归神经网络（允许同一层结点相连，或者连接到前面的各层）。激活函数也变得更多样，例如Sigmoid函数，softmax等等。多层的神经网络是如何学习权值的呢？使用的Back-propagation的方法，反向传播。因为中间层没有确定的类标号，无法计算误差。

那么确定一个多层的神经网络，首先需要确定输入层的结点数，输出层的结点数，网络的拓扑结构（隐藏层结点数、前馈还是递归等等），初始权值和偏置（常常随机赋值，但是有可能陷入局部最小值）。

ANN可以处理冗余信息，但是对噪声非常敏感，而且其梯度下降的算法经常会收敛到局部最小值（在权值更新公式中加入一个动量项来解决），而且这个模型复杂，成本比较高。

## 2.5 支持向量机SVM

支持向量机是近年来工业中越来越常用的一种技术，相比较之前的模型我们首先来介绍一下最大边缘超平面的概念。

作为支持向量机的基础概念，这个概念非常好理解，超平面就是一种线性划分，想象一下两个数据集，如果它区分的足够清晰，我们可以用无数个超平面来将两个数据集分开，那么哪个超平面最好呢？就是最大超平面的概念。最大超平面具有最大的宽度，何谓宽度呢？就是平行于这个超平面向外扩张，最早不能将两类数据划分开的那两个平面之间的垂直距离。那么为什么宽度最大就最好呢？显而易见的是，如果这个宽度越大，当数据集的噪声越大时，越不容易受到干扰（注意这里面的逻辑）。如果他的决策边界的边缘比较小，就很敏感。

那么接下来就说说SVM了。最简单的SVM是一种线性分类器。线性SVM就是在寻找最大边缘超平面。如何寻找呢？我们假设这个最大超平面由一个线性函数决定。

$$w\cdot x+b=0$$

$$w和b$$是模型的参数，$$x$$是训练集。

如果我们将某一类的取值定为1，另一类的取值定为-1，那么可以通过调整参数$$w,b$$是的这个最大超平面的边缘由如下两个方程决定：

$$w\cdot x+b=-1$$

$$w\cdot x^{'} +b =1$$

两个超平面相减就得到边缘 $$d=(x-x^{'})=\frac{2}{\|w\|^2}$$

如何构建SVM模型呢？简单来说就是最大化变换d（因为我们在寻找最大边缘超平面），同时要满足两个条件：

$$w\cdot x_i+b\ge1$$ 如果 $$y_i=1$$

$$w\cdot x_i+b\le-1$$ 如果 $$y_i=-1$$

那么这就变成了一个拉格朗日方程，但是他的求解并没有那么简单，需要转化成对偶问题，利用二次规划的方法求得拉格朗日乘子的值，然后在带回求得参数的值。

这个方法可以解决数据集可以用线性的超平面划分的情况下，如果不可分呢或者说如果只是牺牲了一点错误率而有更好的稳定性我们选择哪一个呢？

我们定义软边缘的SVM其实是更常用的，软边缘就是在边缘方程的条件中加入了松弛变量：

$$w\cdot x_i+b\ge1-\xi_i$$ 如果 $$y_i=1$$

$$w\cdot x_i+b\le-1+\xi_i$$ 如果 $$y_i=-1$$

引入松弛变量的作用就是允许原始的边缘中包含了一些错误的点，那么如果不对$$\xi$$的大小进行约束，那么这个错误率可能会变得很大，如何调整呢？简单，就对目标函数加入一个对$$\xi$$的惩罚项。

$$f(w)=\frac{\|w\|^2}{2}+C\left(\Sigma^N_{i=1}\xi_i\right)^k$$

同样构建拉格朗日方程，转换成对偶问题，二次规划求解拉格朗日乘子，然后再解出参数即可。

以上介绍的都是线性的SVM技术，下面来说说非线性的SVM技术。

讲了这么多线性的情况，突然冒出来一个非线性，是不是很不爽，别担心，我们的思路是将非线性的数据映射到一个新的坐标空间，使得我们可以使用之前线性的方法来解决这个问题。那么这里的重点就转移到了如何做这个属性空间的转换呢？

一个低维非线性的空间可以通过映射到一个高维的空间达到线性的要求，比如低维空间中的圆$$(x+1)^2+(y+1)^2=1$$映射到高维的线性空间$$(x_1^2,x_2^2,x_1,x_2,x_1x_2)$$。可以看出这种方法一个潜在的问题就是可能会产生维灾难！

当当当当当！请叫我解决高维问题的核函数！核技术是一种使用原属性集计算变换后的空间中的相似度的方法。核函数（Kernel Function）是在原属性空间中计算变换后空间中的两个向量相似度（点积）的函数。核函数根据不同的形式可以分为多项式核函数、高斯核函数等。SVM核变换后的空间称作再生核希尔伯特空间（Reproducing Kernel Hilbert Space,RKHS）。使用核函数计算点积（即相似度）的开销更小，而且既然是在原空间内计算，维灾难问题自然就解决了。

这么好的技术为什么不广泛使用呢？当然会有一定的条件啦。这就是Mercer定理。符合Mercer定理就说明，存在一个相应的变换，使得计算一对向量的核函数等价于在变换后的空间中计算这对向量的点积。

> Mercer定理 核函数K可以表示为：
  $$k(U,V)= \Phi(u) \cdot \Phi(v) $$
  当且仅当对于任意满足$$\int g(x)^2dx$$为有限值得函数$$g(x)$$，则
  $$\int K(x,y)g(x)g(y)dxdy \ge0$$

满足Mercer定理的核函数成为正定核函数，例如之前提到的多项式核函数、高斯核函数、Sigmoid核函数。

Ok，支持向量机也差不多圆满了。为什么SVM这么广泛的应用呢，因为它能找到全局最小点！而像神经网络、决策树等分类方法大多使用贪心算法，一般只能获得局部最优点。